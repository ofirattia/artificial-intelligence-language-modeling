{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling - Foundations\n",
    "\n",
    "Assumptions and terminology\n",
    "We will assume that text data is in the form of sentences with no punctuation. If a sentence is in a single line, we will add add a token for start of sentence: `<s> and end of sentence: </s>.` For example, if the sentence is \"I love language models.\" it will appear in code as:\n",
    "\n",
    "`I love language models`\n",
    "The tokens for this sentence are represented as an ordered list of the lower case words plus the start and end sentence tags:\n",
    "\n",
    "tokens = `['<s>', 'i', 'love', 'language', 'models', '</s>']`\n",
    "\n",
    "The bigrams for this sentence are represented as a list of lower case ordered pairs of tokens:\n",
    "\n",
    "bigrams = `[('<s>', 'i'), ('i', 'love'), ('love', 'language'), ('language', 'models'), ('models', '</s>')]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section we have a function that returns a list of tokens and a list of bigrams for a given sentence. You will need to first break a sentence into words in a list, then add a ``<s> and <s/>`` token to the start and end of the list to represent the start and end of the sentence.\n",
    "\n",
    "Your final lists should be in the format shown above and called out in the function doc string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<s>', 'the', 'old', 'man', 'spoke', 'to', 'me', '</s>'],\n",
       " [('<s>', 'the'),\n",
       "  ('the', 'old'),\n",
       "  ('old', 'man'),\n",
       "  ('man', 'spoke'),\n",
       "  ('spoke', 'to'),\n",
       "  ('to', 'me'),\n",
       "  ('me', '</s>')])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    'the old man spoke to me',\n",
    "    'me to spoke man old the',\n",
    "    'old man me old man me',\n",
    "]\n",
    "\n",
    "def sentence_to_bigrams(sentence):\n",
    "    \"\"\"\n",
    "    Add start '<s>' and stop '</s>' tags to the sentence and tokenize it into a list\n",
    "    of lower-case words (sentence_tokens) and bigrams (sentence_bigrams)\n",
    "    :param sentence: string\n",
    "    :return: list, list\n",
    "        sentence_tokens: ordered list of words found in the sentence\n",
    "        sentence_bigrams: a list of ordered two-word tuples found in the sentence\n",
    "    \"\"\"\n",
    "    sentence_tokens = ['<s>'] + sentence.lower().split() + ['</s>']\n",
    "    sentence_bigrams = []\n",
    "    for i in range(len(sentence_tokens)-1):\n",
    "        sentence_bigrams.append((sentence_tokens[i], sentence_tokens[i+1]))\n",
    "    return sentence_tokens, sentence_bigrams\n",
    "\n",
    "sentence_to_bigrams(test_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilities and Likelihoods with Bigrams\n",
    "Recall from a previous video that the probability of a series of words can be calculated from the chained probabilities of its history:\n",
    "\n",
    "\n",
    "The probabilities of sequence occurrences in a large textual corpus can be calculated this way and used as a language model to add grammar and contectual knowledge to a speech recognition system. However, there is a prohibitively large number of calculations for all the possible sequences of varying length in a large textual corpus.\n",
    "\n",
    "To address this problem, we use the Markov Assumption to approximate a sequence probability with a shorter sequence:\n",
    "\n",
    "\n",
    "In the bigram case, the equation reduces to a series of bigram probabilities multiplied together to find the approximate probability for a sentence. A concrete example:\n",
    "\n",
    "\n",
    "We can calculate the probabilities by using counts of the bigramsand individual tokens. The counts are represented below with the c() operator:\n",
    "\n",
    "\n",
    "In Python, the Counter method is useful for this task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 2, 'am': 2, '<s>': 1, 'as': 1, '</s>': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# Sentence: \"I am as I am\"\n",
    "tokens = ['<s>', 'i', 'am', 'as', 'i', 'am', '</s>']\n",
    "token_counts = Counter(tokens)\n",
    "print(token_counts)\n",
    "# output:\n",
    "# Counter({'</s>': 1, '<s>': 1, 'am': 2, 'as': 1, 'i': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0.0,\n",
       " ('<s>', 'the'): 0.3333333333333333,\n",
       " ('the', 'old'): 0.5,\n",
       " ('old', 'man'): 0.75,\n",
       " ('man', 'spoke'): 0.25,\n",
       " ('spoke', 'to'): 0.5,\n",
       " ('to', 'me'): 0.5,\n",
       " ('me', '</s>'): 0.5,\n",
       " ('<s>', 'me'): 0.3333333333333333,\n",
       " ('me', 'to'): 0.25,\n",
       " ('to', 'spoke'): 0.5,\n",
       " ('spoke', 'man'): 0.5,\n",
       " ('man', 'old'): 0.25,\n",
       " ('old', 'the'): 0.25,\n",
       " ('the', '</s>'): 0.5,\n",
       " ('<s>', 'old'): 0.3333333333333333,\n",
       " ('man', 'me'): 0.5,\n",
       " ('me', 'old'): 0.25}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import utils\n",
    "\n",
    "def bigram_mle(tokens, bigrams):\n",
    "    \"\"\"\n",
    "    provide a dictionary of probabilities for all bigrams in a corpus of text\n",
    "    the calculation is based on maximum likelihood estimation and does not include\n",
    "    any smoothing.  A tag '<unk>' has been added for unknown probabilities.\n",
    "    :param tokens: list\n",
    "        tokens: list of all tokens in the corpus\n",
    "    :param bigrams: list\n",
    "        bigrams: list of all two word tuples in the corpus\n",
    "    :return: dict\n",
    "        bg_mle_dict: a dictionary of bigrams:\n",
    "            key: tuple of two bigram words, in order OR <unk> key\n",
    "            value: float probability\n",
    "            \n",
    "    \"\"\"\n",
    "    bg_mle_dict = {}\n",
    "    bg_mle_dict['<unk>'] = 0.\n",
    "\n",
    "    token_raw_counts = Counter(tokens)\n",
    "    bigram_raw_counts = Counter(bigrams)\n",
    "    for bg in bigram_raw_counts:\n",
    "        bg_mle_dict[bg] = bigram_raw_counts[bg] / token_raw_counts[bg[0]]\n",
    "    return bg_mle_dict\n",
    "\n",
    "tokens = []\n",
    "bigrams = []\n",
    "for line in test_sentences:\n",
    "    line_tokens, line_bigrams = sentence_to_bigrams(line)\n",
    "    tokens = tokens + line_tokens\n",
    "    bigrams = bigrams + line_bigrams\n",
    "    \n",
    "bigram_mle(tokens, bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothing and logs\n",
    "There are still a couple of problems to sort out before we use the bigram probability dictionary to calculate the probabilities of new sentences:\n",
    "\n",
    "1. Some possible combinations may not exist in our probability dictionary but are still possible. We don't want to multiply in a probability of 0 just because our original corpus was deficient. This is solved through \"smoothing\". There are a number of methods for this, but a simple one is the Laplace smoothing with the \"add-one\" estimate where `V` is the size of the vocabulary for the corpus, i.e. the number of unique tokens\n",
    "<img src=\"eqn-addone-bigram-smoothing.png\" />\n",
    "\n",
    "2. Repeated multiplications of small probabilities can cause underflow problems in computers when\n",
    "the values become to small. To solve this, we will calculate all probabilities in log space\n",
    "<img src=\"log.png\" />\n",
    "\n",
    "a utility named utils.bigram_add1_logs has been added for you with Laplace smoothing in the log space. Write a function that calculates the log probability for a given sentence, using this log probability dictionary. If all goes well, you should observe that more likely sentences yield higher values for the log probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** \"the old man spoke to me\"\n",
      "-34.80495531345013\n",
      "*** \"me to spoke man old the\"\n",
      "-39.34280606002005\n",
      "*** \"old man me old man me\"\n",
      "-36.59899481268447\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "test_sentences = [\n",
    "    'the old man spoke to me',\n",
    "    'me to spoke man old the',\n",
    "    'old man me old man me',\n",
    "]\n",
    "\n",
    "def sample_run():\n",
    "    # sample usage by test code (this definition not actually run for the quiz)\n",
    "    bigram_log_dict = utils.bigram_add1_logs('transcripts.txt')\n",
    "    for sentence in test_sentences:\n",
    "        print('*** \"{}\"'.format(sentence))\n",
    "        print(log_prob_of_sentence(sentence, bigram_log_dict))\n",
    "\n",
    "def log_prob_of_sentence(sentence, bigram_log_dict):\n",
    "    # get the sentence bigrams\n",
    "    s_tokens, s_bigrams = utils.sentence_to_bigrams(sentence)\n",
    "\n",
    "    # add the log probabilites of the bigrams in the sentence\n",
    "    total_log_prob = 0.\n",
    "    for bg in s_bigrams:\n",
    "        if bg in bigram_log_dict:\n",
    "            total_log_prob = total_log_prob + bigram_log_dict[bg]\n",
    "        else:\n",
    "            total_log_prob = total_log_prob + bigram_log_dict['<unk>']\n",
    "    return total_log_prob\n",
    "\n",
    "sample_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
